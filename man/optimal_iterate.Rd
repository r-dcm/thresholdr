% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimal-iterate.R
\name{optimal_iterate}
\alias{optimal_iterate}
\title{Use iteration to estimate an optimal probability threshold}
\usage{
optimal_iterate(
  estimates,
  weighting_method,
  optimal_method,
  ...,
  additional_criterion = NULL,
  iter_burnin = 100,
  iter_retain = 1000,
  comp_thresholds = NULL,
  metrics = NULL
)
}
\arguments{
\item{estimates}{A vector of probabilities.}

\item{weighting_method}{The method for generating classifications, weighted
by the current estimate of the optimal threshold. One of
"beta", "distance".}

\item{optimal_method}{The method for estimating the optimal threshold. One of
"youden", "topleft", "cz", "gmean".}

\item{...}{Additional arguments passed to the corresponding weighting method.}

\item{additional_criterion}{Optional. If provided, must be a class
probability metric from \link[yardstick:yardstick-package]{yardstick}.}

\item{iter_burnin}{The number of iterations to run and then discard (see
Details below).}

\item{iter_retain}{The number of iterations to retain (see Details below).}

\item{comp_thresholds}{Additional threshold values to evaluate against the
average optimal threshold (e.g., to compare the optimal threshold to a
competing threshold such as 0.5). If \code{NULL} (the default), no additional
thresholds are included in the performance evaluation.}

\item{metrics}{Either \code{NULL} or a \code{\link[yardstick:metric_set]{yardstick::metric_set()}} with a list of
performance metrics to calculate. The metrics should all be oriented
towards hard class predictions (e.g., \code{\link[yardstick:sens]{yardstick::sensitivity()}},
\code{\link[yardstick:accuracy]{yardstick::accuracy()}}, \code{\link[yardstick:recall]{yardstick::recall()}}) and not class
probabilities. A set of default metrics is used when \code{NULL} (see
\code{\link[probably:threshold_perf]{probably::threshold_perf()}} for details).}
}
\value{
A \link[tibble:tibble-package]{tibble} with 1 row per threshold. The
columns are:
\itemize{
\item \code{.threshold}: The optimal threshold.
\item If \code{additional_criterion} was specified, an \link[posterior:rvar]{rvar}
containing the distribution of class probability metrics across all
retained iterations.
\item A set of \link[posterior:rvar]{rvar} objects for each of the specified
performance metrics, containing the distributions across all retained
iterations (i.e., 1 column per specified metric).
}
}
\description{
Use iteration to estimate an optimal probability threshold when true
classifications are unknown.
}
\details{
To initialize the iteration process, a vector of "true" values is generated
using \code{\link[=generate_truth]{generate_truth()}}. Then, the optimal threshold is calculated using the
set of generated "true" values and the specified \code{optimal_method}. A new
vector of "true" values is then generated, with classifications biased in the
direction of the calculated optimal threshold using the method specified by
\code{weighting_method}. That is, \code{estimates} will be less likely to result in a
classification 1 if the threshold is .8 than if it is .5. Using the updated
vector of "true" values, a new optimal threshold is calculated. This proceeds
for the specified number of iterations. The total number of iterations is
given by \code{iter_burnin + iter_retain}; however, the first \code{iter_burnin}
iterations are discarded. For example, if you specify 100 burn-in iterations
and 1,000 retained iterations, a total of 1,100 total iterations will be
completed, but results will be based only on the final 1,000 iterations.
The optimal threshold is then calculated as the average of the threshold
values from the retained iterations.

Convergence of the iteration process is monitored using the \eqn{\hat{R}}
statistic described by Vehtari et al. (2021). By default, the \eqn{\hat{R}}
statistic is calculated for the optimal threshold values that are estimated
at each iteration. Optionally, users may specify and \code{additional_criterion}
to be monitored with the \eqn{\hat{R}}. For example, we could calculate the
area under the ROC curve with the "true" values used at each iteration to
monitor that value for convergence as well. A warning is produced if the
threshold or, if specified, the \code{additional_criterion} do not meet the
convergence criteria of an \eqn{\hat{R}} less than 1.01 recommended by
Vehtari et al. (2021).

Finally, the average threshold is applied to the samples of "true" values
that were generated at each iteration to calculate performance metrics for
each iteration (e.g., sensitivity, specificity). In addition, we can also
specify additional thresholds to compare (\code{comp_thresholds}) that may be of
interest (e.g., comparing our optimal threshold to the traditional threshold
of 0.5). Thus, the final returned object includes each of the investigated
thresholds (i.e., the optimal threshold and any specified in
\code{comp_thresholds}) and the distribution of the performance metrics across all
retained iterations for each of the thresholds. To change the metrics that
are provided by default, specify new \code{metrics}.
}
\examples{
est <- runif(100)
optimal_iterate(estimates = est, weighting_method = "distance",
                optimal_method = "youden", iter_retain = 100)

}
\references{
Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., & BÃ¼rkner,
P.-C. (2021). Rank-normalization, folding, and localization: An improved
\eqn{\hat{R}} for assessing convergence of MCMC (with discussion).
\emph{Bayesian Analysis, 16}(2), 667-718. \doi{10.1214/20-BA1221}
}
\seealso{
Other threshold approximation methods: 
\code{\link{optimal_resample}()}
}
\concept{threshold approximation methods}
